<!DOCTYPE html>
<html lang="zh-CN">

<head>
        <link rel="canonical" href="https://qataraddress.github.io/html/category/article-3445.htm" />
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>基于YARN HA集群的Spark HA集群 - Qatar Address</title>
        <link rel="icon" href="/assets/addons/xcblog/img/qataraddress/favicon.ico" type="image/x-icon"/>
    <!-- Bootstrap -->
    <link href="/assets/addons/xcblog/css/qataraddress/bootstrap.min.css" rel="stylesheet">
    <!-- Style CSS -->
    <link href="/assets/addons/xcblog/css/qataraddress/style.css" rel="stylesheet">
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Fira+Sans:100,100i,200,200i,300,300i,400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">
    <link href="/assets/addons/xcblog/css/qataraddress/owl.carousel.css" rel="stylesheet">
    <link href="/assets/addons/xcblog/css/qataraddress/owl.theme.default.css" rel="stylesheet">
    <!-- FontAwesome CSS -->
    <link rel="stylesheet" type="text/css" href="/assets/addons/xcblog/css/qataraddress/fontello.css">
    <link href="/assets/addons/xcblog/css/qataraddress/font-awesome.min.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f2cd1f435343615e6078c743d0623024";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
</head>

<body>
        <!-- top-bar -->
    <div class="top-bar">
        <div class="container">
            <div class="row">
                <div class="col-lg-3 col-md-3 col-sm-3 col-xs-12">
                    <div class="logo">
                                                <a href="/">Qatar Address</a>
                                            </div>
                </div>
            </div>
        </div>
    </div>
    <!-- /.top-bar -->
    <!-- header-section-->
    <div class="header-wrapper">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                    <!-- navigations-->
                    <div class="navigation">
                        <div id="navigation">
                            <ul>
                                                                <li><a href="/">首页</a></li>
                                                                <li><a href="/html/category/">文章分类</a></li>
                                                                <li><a href="#">关于</a></li>
                                <li><a href="#">联系</a>
                                </li>
                            </ul>
                        </div>
                    </div>
                    <!-- /.navigations-->
                </div>
            </div>
        </div>
    </div>
    <!-- /. header-section-->
    <!-- page-header -->
    <div class="page-header">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-10 col-xs-12">
                    <div class="page-section">
                        <h1 class="page-title">基于YARN HA集群的Spark HA集群</h1>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                    <div class="page-breadcrumb">
                        <ol class="breadcrumb">
                            <li><a href="/">首页</a></li>
                            <li><a href="/html/category/">文章分类</a></li>
                            <li>正文</li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- /.page-header-->
    <!-- /.page-header-->
    <div class="space-medium">
        <div class="container">
            <div class="row">
                <div class="col-md-9">
                      				  				  				<div id="content_views" class="markdown_views prism-atom-one-light"> <div class="toc"> <h3>文章目录</h3> <ul> <li> <ul> <li> <ul> <li>前言</li> <li>1、yarn HA模式的配置</li> <li> <ul> <li>1.1 完整 yarn-site.xml配置</li> <li>1.2 mapred-site.xml的配置文件说明</li> <li>1.3 yarn HA的启动</li> </ul> </li> <li>2、spark HA 集群及其基本测试</li> <li> <ul> <li>2.1 修改spark配置</li> <li>2.2 启动spark集群</li> </ul> </li> <li>3、spark on yarn</li> <li> <ul> <li>3.1 spark集群跑在yarn上的两种方式</li> <li>3.2 测试spark on yarn</li> <li>3.3 提交spark application的多种方式</li> </ul> </li> <li>4、小结</li> </ul> </li> </ul> </li> </ul> </div> <h3>前言</h3> <p>  在前面的《基于hadoop3.1.2分布式平台上部署spark HA集群》，这篇是基于非HA模式下hadoop集群的spark集群HA配置，而本文将给出基于HA模式下hadoop集群的spark集群HA配置，并将yarn HA集群映入到spark中，做资源管理。为何要做些环境的配置呢？因为到本篇文章为止，已经完成hadoop HA集群、hbaseHA集群，hive集群（非HA）、sparkHA集群、flumeHA集群、kafka HA集群，实现实时数据流动，接下的文章重点探讨spark streaming、spark以及pyspark相关知识，这将涉及多个计算任务以及相关计算资源的分配，因此需要借助yarn HA集群强大的资源管理服务来管理spark的计算任务，从而实现完整的、接近生产环境的、HA模式下的大数据实时分析项目的架构。<br /> 服务器资源分配表(仅列出yarn和spark)：</p> <table> <thead> <tr> <th>节点</th> <th>yarn 角色</th> <th>spark 角色</th> </tr> </thead> <tbody> <tr> <td>nn</td> <td>ResourceManager， NodeManager</td> <td>Master，Worker</td> </tr> <tr> <td>dn1</td> <td>NodeManager</td> <td>Worker</td> </tr> <tr> <td>dn2</td> <td>ResourceManager， NodeManager</td> <td>Master，Worker</td> </tr> </tbody> </table> <p>  这里再提下yarn管理大数据集群计算中对资源有效管理（主要指CPU、物理内存以及虚拟内存）的重要性：</p> <blockquote> <p>  整个集群的计算任务由ResourceManager和NodeManager共同完成，其中，ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为计算任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。</p> </blockquote> <p>因为spark就是负责计算，有大量计算任务要运行，每个任务总得分配cpu和内存给它用，否则某些计算任务会被“饿死”（巧妇难为无米之炊），这种比喻比较形象。</p> <h3>1、yarn HA模式的配置</h3> <p>yarn HA模式的运行是于hadoop HA模式运行的，关于hadoop HA部署和测试可以参考本博客文章《基于Hadoop HA集群部署HBase HA集群（详细版）》的第6章内容，考虑到后面文章将会给出各种spark计算任务，结合测试服务器本身cpu和内存资源有限，这里主要重点介绍yarn-site.xml和mapred-site.xml配置文件说明。</p> <h4>1.1 完整 yarn-site.xml配置</h4> <p>yarn-site的配置其实分为两大块：第一部分为yarn HA集群的配置，第二部分为根据现有测试服务器资源来优化yarn配置。<br /><mark>yarn-site.xml在三个节点上都使用相同配置，无需更改</mark><br /> 第一部分：yarn HA集群的配置<br /> （注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code><configuration></configuration></code>）</p> <pre><code class="prism language-shell"><span class="token operator"><</span><span class="token operator">!</span>-- 启用yarn HA高可用性 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.ha.enabled<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>true<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 指定resourcemanager的名字，自行命名，跟服务器hostname无关 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.cluster-id<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>hayarn<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.ha.rm-ids<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>rm1,rm2<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 指定nn节点为rm1 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.hostname.rm1<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>nn<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 指定dn2节点为rm2  --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.hostname.rm2<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>dn2<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 指定当前机器nn作为主rm1 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.ha.id<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>rm1<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 指定zookeeper集群机器 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.resourcemanager.zk-address<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>nn:2181,dn1:2181,dn2:2181<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.nodemanager.aux-services<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>mapreduce_shuffle<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span></code></pre> <p>以上将nn和dn2作为yarn集群主备节点，对应的id为rm1、rm2</p> <p>第二部分：yarn的优化配置<br /> A、禁止检查每个任务正使用的物理内存量、虚拟内存量是否可用<br /> 若任务超出分配值，则将其杀掉。考虑到作为测试环境，希望看到每个job都能正常运行，以便记录其他观测事项，这里将其关闭。</p> <pre><code class="prism language-shell"><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.nodemanager.pmem-check-enabled<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>false<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.nodemanager.vmem-check-enabled<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>false<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span></code></pre> <p>B、配置RM针对单个Container能申请的最大资源或者RM本身能配置的最大内存<br /> 配置解释：单个容器可申请的最小与最大内存，Application在运行申请内存时不能超过最大值，小于最小值则分配最小值，例如在本文测试中，因计算任务较为简单，无需太多资源，故最小值设为512M，最大值设为1024M。注意最大最不小于1G，因为yarn给一个executor分配512M时，还需要另外动态的384M内存（Required executor memory (512), overhead (384 MB)）。</p> <pre><code class="prism language-shell"><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.scheduler.minimum-allocation-mb<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>512<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.scheduler.maximum-allocation-mb<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>1024<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span></code></pre> <p>若将yarn.scheduler.maximum-allocation-mb设为例如512M，spark on yarn就会启动失败。</p> <p>C、NM的内存资源配置，主要是通过下面两个参数进行的</p> <p>第一个参数：每个节点可用的最大内存，默认值为-1，代表着yarn的NodeManager占总内存的80%，本文中，物理内存为1G</p> <p>第二个参数：NM的虚拟内存和物理内存的比率，默认为2.1倍</p> <pre><code class="prism language-shell"><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.nodemanager.resource.memory-mb<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>1024<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>nm向本机申请的最大物理内存，默认8G<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.nodemanager.vmem-pmem-ratio<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>3<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span></code></pre> <p>vmem-pmem-ratio的默认值为2.1，由于本机器中，每个节点的物理内存为1G，因此单个RM拿到最大虚拟内存为2.1G，例如在跑spark任务，会出现<code>2.5 GB of 2.1 GB virtual memory used. Killing container</code>的提示，Container申请的资源为2.5G，已经超过默认值2.1G，当改为3倍时，虚拟化够用，故解决可该虚拟不足的情况。</p> <h4>1.2 mapred-site.xml的配置文件说明</h4> <p>mapred-site的配置其实分为两大块：第一部分为mapreduce的基本配置，第二部分为根据现有测试服务器资源来优化mapreduce计算资源分配的优化配置。<br /><mark>mapred-site.xml在三个节点上都需要配置，只需把nn主机名改为当前节点的主机名即可</mark><br /> 第一部分：mapreduce的基本配置<br /> （注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code><configuration></configuration></code>）</p> <pre><code class="prism language-shell"><span class="token operator"><</span><span class="token operator">!</span>-- 使用yarn框架来管理MapReduce --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.framework.name<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>yarn<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- mp所需要hadoop环境 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.app.mapreduce.am.env<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>HADOOP_MAPRED_HOME<span class="token operator">=</span><span class="token variable">${HADOOP_HOME}</span><span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.map.env<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>HADOOP_MAPRED_HOME<span class="token operator">=</span><span class="token variable">${HADOOP_HOME}</span><span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.reduce.env<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>HADOOP_MAPRED_HOME<span class="token operator">=</span><span class="token variable">${HADOOP_HOME}</span><span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 打开Jobhistory --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.jobhistory.address<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>nn:10020<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>-- 指定nn作为jobhistory服务器 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.jobhistory.webapp.address<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>nn:19888<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>--存放已完成job的历史日志 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.jobhistory.done-dir<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>/history/done<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>--存放正在运行job的历史日志 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.jobhistory.intermediate-done-dir<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>/history/done_intermediate<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span><span class="token operator">!</span>--存放yarn stage的日志 --<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>yarn.app.mapreduce.am.staging-dir<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>/history/staging<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span></code></pre> <p>这里主要配置开启jobhistory服务以及MapReduce多种日志存放</p> <p>第二部分：mapreduce的优化项</p> <pre><code class="prism language-shell"><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.map.memory.mb<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>100<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>每个mapper任务的物理内存限制<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.reduce.memory.mb<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>200<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>每个reducer任务的物理内存限制<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.map.cpu.vcores<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>1<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>每个mapper任务申请的虚拟cpu核心数，默认1<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.reduce.cpu.vcores<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>1<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>每个reducer任务申请的虚拟cpu核心数，默认1<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.map.java.opts<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>-Xmx100m<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>mapper阶段的JVM的堆大小<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span><span class="token operator"><</span>property<span class="token operator">></span><span class="token operator"><</span>name<span class="token operator">></span>mapreduce.reduce.java.opts<span class="token operator"><</span>/name<span class="token operator">></span><span class="token operator"><</span>value<span class="token operator">></span>-Xmx200m<span class="token operator"><</span>/value<span class="token operator">></span><span class="token operator"><</span>description<span class="token operator">></span>reduce阶段的JVM的堆大小<span class="token operator"><</span>/description<span class="token operator">></span><span class="token operator"><</span>/property<span class="token operator">></span></code></pre> <p>根据当前服务器物理配置资源，在内存和CPU方面给mapper和reducer任务进行调优。</p> <h4>1.3 yarn HA的启动</h4> <p>首先确保hadoop HA集群已正常启动</p> <pre><code>[root@nn sbin]# hdfs haadmin -getServiceState nn active [root@nn sbin]# hdfs haadmin -getServiceState dn2 standby</code></pre> <p>启动yarn HA服务，只需在nn节点启动yarn后，其他节点会自动启动相应服务。</p> <pre><code>[root@nn sbin]# start-yarn.sh  [root@nn sbin]# yarn rmadmin -getServiceState rm1 active [root@nn sbin]# yarn rmadmin -getServiceState rm2 standby</code></pre> <p>以上完成yarn HA配置，因为涉及hadoop HA和调优，因此不建议刚入门的同学就按此配置继续测试，建议从最原始、最简单的非HA hadoop开始着手。<br /> 下面开始配置spark。</p> <h3>2、spark HA 集群及其基本测试</h3> <h4>2.1 修改spark配置</h4> <p>  经历第1章节繁琐的yarn HA配置后， 当资源管理问题得到妥善解决，那么接下的计算任务将实现的非常流畅。<br /> spark HA集群详细的部署和测试，请参考《基于hadoop3.1.2分布式平台上部署spark HA集群》的第8章节，本文不再累赘。<br />   把spark 的任务交给yarn管理还需要在HA集群上再加入部分配置，改动也简单 ，只需在spark-defaults.conf和spark-env.sh改动。</p> <pre><code>[root@nn conf]# pwd /opt/spark-2.4.4-bin-hadoop2.7/conf [root@nn conf]# vi spark-defaults.conf  #spark.master                     spark://nn:7077 spark.eventLog.enabled           true  # spark.eventLog.dir               hdfs://nn:9000/directory spark.eventLog.dir               hdfs://hdapp/directory spark.serializer                 org.apache.spark.serializer.KryoSerializer spark.driver.memory              512m spark.driver.cores               1 spark.yarn.jars                  hdfs://hdapp/spark_jars/* spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"</code></pre> <p>重点配置项目说明：<br /> #spark.master spark://nn:7077<br /> 因为spark已经配成HA模式，因此无需指定master是谁，交由zookeeper管理。</p> <p>spark.eventLog.dir hdfs://hdapp/directory<br /> 这里hdfs路径从nn:9000改为hdapp，是因为hadoop已经配置为HA模式，注意集群模式下是不需要加上端口： hdfs://hdapp:9000/directory，这会导致NameNode无法解析host部分。</p> <p>spark.yarn.jars hdfs://hdapp/spark_jars/*<br /> 这里需要将spark跟目录下的jar包都上传到hdfs指定的spark_jars目录下，若不这么处理，每次提交spark job时，客户端每次得先上传这些jar包到hdfs，然后再分发到每个NodeManager，导致任务启动很慢。而且启动spark也会提示：<br /><mark>WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</mark></p> <p>解决办法：</p> <pre><code>[root@nn spark-2.4.4-bin-hadoop2.7]# pwd /opt/spark-2.4.4-bin-hadoop2.7 [root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -mkdir  /spark_jars [root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -put  jars/*  /spark_jars</code></pre> <p>spark-defaults.conf在三个节点上使用相同配置。</p> <p>spark-env.sh的配置：</p> <pre><code>[root@nn conf]# pwd /opt/spark-2.4.4-bin-hadoop2.7/conf [root@nn conf]# vi spark-env.sh # 基本集群配置 export SCALA_HOME=/opt/scala-2.12.8 export JAVA_HOME=/opt/jdk1.8.0_161 export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=nn:2181,dn1:2181,dn2:2181 -Dspark.deploy.zookeeper.dir=/spark" export HADOOP_CONF_DIR=/opt/hadoop-3.1.2/etc/hadoop  # yarn模式下的调优配置 # Options read in YARN client/cluster mode export SPARK_WORKER_MEMORY=512M # - SPARK_CONF_DIR, Alternate conf dir. (Default: ${SPARK_HOME}/conf) 无需设置，使用默认值 # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files export HADOOP_CONF_DIR=/opt/hadoop-3.1.2/etc/hadoop # - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN 上面HADOOP_CONF_DIR以已设置即可 # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1). 无需设置，默认使用1个vcpu # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G) export SPARK_EXECUTOR_MEMORY=512M # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G) export SPARK_EXECUTOR_MEMORY=512M  # 存放计算过程的日志 export SPARK_HISTORY_OPTS=" -Dspark.history.ui.port=9001 -Dspark.history.retainedApplications=5 -Dspark.history.fs.logDirectory=hdfs://hdapp/directory"</code></pre> <p>以上的driver和executor的可用内存设为512M，考虑到测试服务器内存有限的调优。若生产服务器，一般32G或者更大的内存，则可以任性设置。</p> <h4>2.2 启动spark集群</h4> <p>在nn节点上，启动wokers： start-slaves.sh，该命令自动启动其他节点的worker<br /> 在nn节点和dn2节点启动master进程：start-master.sh<br /> 查看nn:8080和dn2:8080的spark web UI是否有active以及standby模式。<br /> 跑一个wordcount例子，测试spark集群能否正常计算结果。<br /> 创建一个本地文件</p> <pre><code>[root@nn spark-2.4.4-bin-hadoop2.7]#  vi /opt/foo.txt spark on yarn yarn  spark HA</code></pre> <p>启动pyspark，连接到spark集群</p> <pre><code>[root@nn spark-2.4.4-bin-hadoop2.7]#  ./bin/pyspark --name bar --driver-memory 512M   --master  spark://nn:7077 # 读取本地文件/opt/foo.txt >>> df=sc.textFile("file:///opt/foo.txt") # 切分单词，过滤空值 >>> words = df.flatMap(lambda line: line.split(' ')).filter(lambda x: x !="") >>> words.collect() [u'spark', u'on', u'yarn', u'yarn',u'spark', u'HA'] # 将个word映射为（word，1）这样的元组，在reduce汇总。 >>> counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) >>> counts.collect() [(u'spark', 2), (u'yarn', 2), (u'on', 1), (u'HA', 1)]</code></pre> <p>以上完成spark HA集群和测试。</p> <h3>3、spark on yarn</h3> <p>spark on yarn意思是将spark计算人任务提交到yarn集群上运行。</p> <h4>3.1 spark集群跑在yarn上的两种方式</h4> <p>根据spark官网的文档说明，这里引用其内容：</p> <blockquote> <p>There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p> </blockquote> <p>cluster模式下，spark driver 在 AM里运行，客户端（或者应用程序）在提交完任务（初始化）后可直接退出，作业会继续在 YARN 上运行。显然cluster 模式不适合交互式操作。cluster模式的spark计算结果可以保持到<br /> 外部数据库，例如hbase。这部分内容将是spark streaming可以完成的环境，spark streaming以yarn cluster模式运行，实时将处理结果存到hbase里，web BI 应用再从hbase取数据。</p> <p>client模式下，spark driver是在本地环境运行，AM仅负责向yarn请求计算资源（Executor 容器），例如交互式运行基本的操作。</p> <p>在前面第2节的word count例子里，用下面的启动命令：</p> <pre><code>[root@nn spark-2.4.4-bin-hadoop2.7]# pyspark --name bar --driver-memory 512M   --master  spark://nn:7077</code></pre> <p>该命令启动是一个spark shell进程，没有引入yarn管理其资源，因此在yarn集群的管理页面http://nn:8088/cluster/apps/RUNNING，将不会 bar这个application。</p> <h4>3.2 测试spark on yarn</h4> <p>只需在启动spark shell时，将<code>--master spark://nn:7077</code> 改为<br /><code>--master yarn --deploy-mode cluster</code>或者<code>--master yarn --deploy-mode client</code>，那么spark提交的任务就会交由yarn集群管理<br /> 还是以word count为例，使用yarn client模式启动spark<br /> 创建测试文件：</p> <pre><code>vi /opt/yarn-word-count.txt spark on yarn  spark HA  yarn HA</code></pre> <p>启动driver</p> <pre><code>[root@nn spark-2.4.4-bin-hadoop2.7]#  pyspark --name client_app	 --driver-memory 512M  --executor-memory 512M  --master yarn --deploy-mode client Python 2.7.5 (default, Oct 30 2018, 23:45:53)  [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2 Welcome to       ____              __      / __/__  ___ _____/ /__     _\ \/ _ \/ _ `/ __/  '_/    /__ / .__/\_,_/_/ /_/\_\   version 2.4.4       /_/  Using Python version 2.7.5 (default, Oct 30 2018 23:45:53) SparkSession available as 'spark'. >>> sc <SparkContext master=yarn appName=client_app	></code></pre> <p>这里driver和executor都是以最小可用内存512来启动spark-shell<br /> 因为该spark 任务是提交到yarn 上运行，所以在spark web ui后台：http://nn:8080，running application 为0<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220829/e92fd1a46d802001e2c16ce54d8cd7e4.jpg" alt="基于YARN HA集群的Spark HA集群">这是需要去yarn后台入口：http://nn:8088，可以看到刚提交的计算任务：<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220829/3da59d23074ccad29ef6ad2425a8f938.jpg" alt="基于YARN HA集群的Spark HA集群">可以看到该application（计算任务）分配了3个Container<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220829/2122662734e623b869b478da64cbee6b.jpg" alt="基于YARN HA集群的Spark HA集群">通过查看该applicationMaster管理页面，可以看到client-yarn这个app更为详细的计算过程，例如该wordcount在reduceByKey DAG可视化过程。<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220829/9f8ec00b711980e103b7eb0918e06260.jpg" alt="基于YARN HA集群的Spark HA集群"><br /> yarn cluster模式下，因为它不是打开一个spark shell让你交互式输入数据处理逻辑，所以需先把处理逻辑封装成一个py模块。<br /> 以上面的word count为例：<br /> word_count.py</p> <pre><code class="prism language-python"><span class="token keyword">from</span> pyspark<span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext<span class="token keyword">def</span><span class="token function">word_count</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 	    conf<span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">'cluster-yarn'</span><span class="token punctuation">)</span> 	    sc<span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span><span class="token comment"># 统计文件中包含mape的行数，并打印第一行</span> 	    df<span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"/tmp/words.txt"</span><span class="token punctuation">)</span> 	    words<span class="token operator">=</span> df<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token operator">!=</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token keyword">print</span> words<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span> 	    counts<span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a<span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token keyword">print</span> counts<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span> 	    sc<span class="token punctuation">.</span>stop<span class="token keyword">if</span> __name__<span class="token operator">==</span><span class="token string">'__main__'</span><span class="token punctuation">:</span> 	word_count<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre> <p>需要使用spark-submit 提交到yarn</p> <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@dn2 spark-2.4.4-bin-hadoop2.7<span class="token punctuation">]</span><span class="token comment">#  ./bin/spark-submit  --driver-memory 512M  --executor-memory 512M  --master yarn  --deploy-mode cluster  --py-files word_count.py</span></code></pre> <p>在yarn管理也可以看到该app，application的命名好像直接用脚本名字，而不是指定的cluster-yarn<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220829/ea6cf4fe9ccf6b1ae91d397e5fdf6ec3.jpg" alt="基于YARN HA集群的Spark HA集群">关于如何提交py文件，官方也给出指引：</p> <blockquote> <p>For Python, you can use the --py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application. If you depend on multiple Python files we recommend packaging them into a .zip or .egg.</p> </blockquote> <p>如有多个py文件（例如1.py依赖2.py和3.py），需要通过将其打包为.zip或者.egg包： --py-files tasks.zip</p> <h4>3.3 提交spark application的多种方式</h4> <p>spark运行有standalone模式（分local、cluster）、on yarn模式（分client、cluster）还有on k8s，而且可以附带jar包或者py包，多种提交的方式的命令模板怎么写？网上其实很多类似文章，但都是给的某个模式的某种文件的提交方式，其实在spark官网的submitting-applications章节给出详细的多种相关命令模板。这里统一汇总：</p> <pre><code class="prism language-shell"><span class="token comment"># Run application locally on 8 cores 本地模式</span> ./bin/spark-submit \   --class org.apache.spark.examples.SparkPi \   --master local<span class="token punctuation">[</span>8<span class="token punctuation">]</span> \   /path/to/examples.jar \   100<span class="token comment">#  standalone 集群下的client模式</span><span class="token comment"># Run on a Spark standalone cluster in client deploy mode</span> ./bin/spark-submit \   --class org.apache.spark.examples.SparkPi \   --master spark://207.184.161.138:7077 \   --executor-memory 20G \   --total-executor-cores 100 \   /path/to/examples.jar \   1000<span class="token comment">#  standalone 集群下的cluster模式</span><span class="token comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span> ./bin/spark-submit \   --class org.apache.spark.examples.SparkPi \   --master spark://207.184.161.138:7077 \   --deploy-mode cluster \   --supervise \   --executor-memory 20G \   --total-executor-cores 100 \   /path/to/examples.jar \   1000<span class="token comment"># on yarn 集群，且用的class文件和jar包</span><span class="token comment"># Run on a YARN cluster</span><span class="token function">export</span> HADOOP_CONF_DIR<span class="token operator">=</span>XXX ./bin/spark-submit \   --class org.apache.spark.examples.SparkPi \   --master yarn \   --deploy-mode cluster \<span class="token comment"># can be client for client mode</span>   --executor-memory 20G \   --num-executors 50 \   /path/to/examples.jar \   1000<span class="token comment"># 这里给出如何传入py文件，可以不写 --py-files 选项</span><span class="token comment"># Run a Python application on a Spark standalone cluster</span> ./bin/spark-submit \   --master spark://207.184.161.138:7077 \   examples/src/main/python/pi.py \   1000<span class="token comment"># Run on a Mesos cluster in cluster deploy mode with supervise</span> ./bin/spark-submit \   --class org.apache.spark.examples.SparkPi \   --master mesos://207.184.161.138:7077 \   --deploy-mode cluster \   --supervise \   --executor-memory 20G \   --total-executor-cores 100 \   http://path/to/examples.jar \   1000<span class="token comment"># Run on a Kubernetes cluster in cluster deploy mode</span> ./bin/spark-submit \   --class org.apache.spark.examples.SparkPi \   --master k8s://xx.yy.zz.ww:443 \   --deploy-mode cluster \   --executor-memory 20G \   --num-executors 50 \   http://path/to/examples.jar \   1000</code></pre> <h3>4、小结</h3> <p>  本文内容主要为后面的文章——spark streaming 与kafka集群的实时数据计算做铺垫，考虑到测试环境环境资源有限，在做spark streaming的时候，将不会以spark HA模式运行，也不会将任务提交到yarn集群上，而是用一节点作为spark streaming计算节点，具体规划参考该文。</p> </div> 			
                    <div class="clearfix"></div>
                    <div class="col-md-12 mt-5">
                                                <p>上一个：<a href="/html/category/article-3422.htm">vue-cli项目添加骨架屏多种方式，自动生成骨架屏</a></p>
                                                <p>下一个：<a href="/html/category/article-3446.htm">CSS3 弹性布局弹性流（flex-flow）属性详解和实例</a></p>
                                            </div>
                                        <div class="panel panel-default mt-5">
                        <div class="panel-heading">
                            <h3 class="m-0">推荐文章</h3>
                        </div>
                        <div class="panel-body">
                            <ul class="p-0 x-0">
                                                                <li class="py-2"><a href="/html/category/article-3402.htm">微信小程序自定义顶部导航栏封装</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3397.htm">「Spring Boot 系列」05. Spring Boot Profiles（多环境配置）</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3392.htm">Python置换操作浅析(a, b=b, a)【Python】</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3373.htm">Centos7服务器上RabbitMQ单机安装_在线工具</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3354.htm">vue中组件之间调用方法</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3340.htm">FP-Growth挖掘频繁项，java实现</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3310.htm">Docker 操作 删除临时镜像（虚悬镜像）</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3289.htm">基于kubernetes的分布式限流</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3256.htm">Nginx离线安装方法详解</a></li>
                                                                <li class="py-2"><a href="/html/category/article-3155.htm">消息队列与快递柜之间的奇妙关系</a></li>
                                                            </ul>
                        </div>
                    </div>
                                    </div>
                <div class="col-md-3">
                    <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/html/category/article-7235.htm" title="宠物疫苗猫多少钱一针啊图片（宠物猫疫苗是什么疫苗）">宠物疫苗猫多少钱一针啊图片（宠物猫疫苗是什么疫苗）</a></li>
                        <li class="py-2"><a href="/html/category/article-5951.htm" title="如何开宠物诊所（开办宠物诊所的条件）">如何开宠物诊所（开办宠物诊所的条件）</a></li>
                        <li class="py-2"><a href="/html/category/article-7465.htm" title="疫苗的分类不包括动物疫苗还是重组疫苗（疫苗分类包括重组疫苗吗）">疫苗的分类不包括动物疫苗还是重组疫苗（疫苗分类包括重组疫苗吗）</a></li>
                        <li class="py-2"><a href="/html/category/article-7418.htm" title="合肥宠物狗领养地址（合肥宠物狗领养地址在哪里）">合肥宠物狗领养地址（合肥宠物狗领养地址在哪里）</a></li>
                        <li class="py-2"><a href="/html/category/article-5977.htm" title="古人类科技（迷茫管家与懦弱的我2）这样也行？，">古人类科技（迷茫管家与懦弱的我2）这样也行？，</a></li>
                        <li class="py-2"><a href="/html/category/article-6127.htm" title="宠物粮生产配方 宠物粮生产配方大全">宠物粮生产配方 宠物粮生产配方大全</a></li>
                        <li class="py-2"><a href="/html/category/article-6729.htm" title="可以自己买疫苗去宠物店打吗（可以自己买疫苗去宠物店打吗现在）">可以自己买疫苗去宠物店打吗（可以自己买疫苗去宠物店打吗现在）</a></li>
                        <li class="py-2"><a href="/html/category/article-6959.htm" title="申贤俊老婆图片（这都可以）申贤俊老婆结婚照，演员阳光，申贤俊，">申贤俊老婆图片（这都可以）申贤俊老婆结婚照，演员阳光，申贤俊，</a></li>
                        <li class="py-2"><a href="/html/category/article-7556.htm" title="动物疫苗打一次管多长时间啊多少钱 动物疫苗打一次管多长时间啊多少钱啊">动物疫苗打一次管多长时间啊多少钱 动物疫苗打一次管多长时间啊多少钱啊</a></li>
                        <li class="py-2"><a href="/html/category/article-6215.htm" title="宠物店卖狗粮的利润是多少（宠物店卖狗粮的利润是多少啊）">宠物店卖狗粮的利润是多少（宠物店卖狗粮的利润是多少啊）</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">20</span> <a href="/html/date/2024-08/" title="2024-08 归档">2024-08</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">61</span> <a href="/html/date/2024-07/" title="2024-07 归档">2024-07</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">60</span> <a href="/html/date/2024-06/" title="2024-06 归档">2024-06</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">61</span> <a href="/html/date/2024-05/" title="2024-05 归档">2024-05</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">60</span> <a href="/html/date/2024-04/" title="2024-04 归档">2024-04</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">62</span> <a href="/html/date/2024-03/" title="2024-03 归档">2024-03</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">44</span> <a href="/html/date/2024-02/" title="2024-02 归档">2024-02</a></h4>
            </li>
                    </ul>
    </div>
</div>


                </div>
            </div>
        </div>
    </div>
        <!-- footer -->
    <div class="footer">
        <!-- tiny-footer -->
        <div class="container">
            <div class="row">
                <div class="tiny-footer">
                    <div class="col-lg-6 col-md-6 col-sm-6 col-xs-12">
                        <p class="tiny-footer-para">
                            Qatar Address 版权所有 Powered by WordPress
                        </p>
                    </div>
                    <div class="col-lg-6 col-md-6 col-sm-6 col-xs-12">
                        <div class="tiny-section-social">
                            <div class="">
                                <ul>
                                    <li><a href="#"><i class="fa fa-facebook-square"></i></a></li>
                                    <li><a href="#"><i class="fa fa-twitter-square"></i></a></li>
                                    <li><a href="#"><i class="fa fa-google-plus-square"></i></a></li>
                                    <li><a href="#"><i class="fa fa-instagram"></i></a></li>
                                    <li><a href="#"><i class="fa fa-linkedin-square"></i></a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- /. tiny-footer -->
        </div>
    </div>
    <!-- /.footer -->
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="/assets/addons/xcblog/js/frontend/qataraddress/jquery.min.js" type="text/javascript"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/assets/addons/xcblog/js/frontend/qataraddress/bootstrap.min.js" type="text/javascript"></script>
    <script src="/assets/addons/xcblog/js/frontend/qataraddress/menumaker.js" type="text/javascript"></script>
    <script type="text/javascript" src="/assets/addons/xcblog/js/frontend/qataraddress/jquery.sticky.js"></script>
    <script type="text/javascript" src="/assets/addons/xcblog/js/frontend/qataraddress/sticky-header.js"></script>
    <script type="text/javascript" src="/assets/addons/xcblog/js/frontend/qataraddress/owl.carousel.min.js"></script>
    <script type="text/javascript" src="/assets/addons/xcblog/js/frontend/qataraddress/multiple-carousel.js"></script>
    <script>
    $(function() {
        $('.js_to').click(function(){
            var url = $(this).data('url');
            var code = $(this).data('code');
            url += code;

            window.open(url);
        })
    });
    </script>
</body>

</html>